\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,bookmark,mathtools,parskip,custom}
\usepackage[margin=.8in]{geometry}
\allowdisplaybreaks
\hypersetup{colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\setcounter{secnumdepth}{5}

\newcommand{\laplace}[1]{\ensuremath{\mathcal{L} \{#1\}}}
\newcommand{\invlaplace}[1]{\ensuremath{\mathcal{L}^{-1} \{#1\}}}

\begin{document}

\title{SE 380 --- Introduction to Feedback Control}
\author{Kevin James}
\date{\vspace{-2ex}Fall 2015}
\maketitle\HRule

\tableofcontents
\newpage

\section{Introduction}
There are two main types of control:
\begin{description}
\item[Open-Loop control] simply converts reference inputs to control inputs (through a controller), then combines these with disturbance inputs in the plant to provide outputs.
\item[Feedback control] adds a second susbsytem, where outputs are fed back into a sensor which measures these outputs, compares them to the reference input, and generates an error signal (which we attempt to minimize)
\end{description}

Disturbance inputs are typically unmeasured and often unmeasurable.

\subsection{Potential Advantages of Feedback Control}
\begin{description}
\item[Tracking] can make output follow reference input.
\item[Regulation] compensates for disturbance inputs.
\item[Robustness] compensates for variation in plant dynamics.
\item[Stabilization] can potentiallially stabilize unsafe plants.
\end{description}

\section{Signals and Systems}
A {\bf signal} is a real- or complex-valued function of a real variable $t$. $t$ usually stands for time, eg. $r(t)$ is the reference input at some time. If the domain of the signal is $\mathbb{R}$ (or some interval of $\mathbb{R}$), then we say that the signal is continuous-time (CT). If the domain is a discrete set (say, $\mathbb{Z}$), the signal is discrete-time (DT).

Mathematically, a {\bf system} is a mapping from a class $f$ of input signals to a class $y$ of output signals.

Notation:
\begin{align*}
f &\xrightarrow{S} y \\
y(t) &= S\bigl(f(t)\bigl) \\
y &= Sf
\end{align*}

If the inputs and outputs are all CT signals, the system is CT. If the inputs and outputs are DT, the system is DT. If there are both CT and DT signals, the system is hybrid.

A CT sytem might be moddels using linear ODEs with constant coefficients. A DT system might be modelled using difference equations. A hyrbid feedback system may use analog-to-digital and digital-to-analog converters to switch between the signal types.

Properties of all systems:
\begin{enumerate}
\item CT, DT, or hybrid
\item Memoryless (statis) or dynamic
\item Causality
\item Multivariable or scalar
\item Linearity
\item Time-invariance
\end{enumerate}

At a given time $t_0$, a memoryless system $y(t_0) = (Sf)(t_0)$ depends only on $f(t_0)$. A system is {\bf dynamic} if it is not memoryless.

Example:
\begin{align*}
M\ddot y(t) &= f(t) \\
\ddot y &= \frac{1}{M} f(t) \\
y(t) &= \frac{1}{M} \int_{-\infty}^t \int_{-\infty}^\tau f(\theta) \dd\theta \dd \tau
\end{align*}

$S$ is {\bf causal} if $y(t) = (Sf)(t)$ depends only on $\{ f(\tau) : \tau \leq t \}$, ie. only on past and present values of $f()$. In other words, if $f_1(\tau) = f_2(\tau) \forall \tau \leq t$ and if $y_1 = Sf_1$ and $y_2 = Sf_2$, then $y_1(\tau) = y_s(\tau) \forall \tau \leq t$.

We can think of causal systems as ``real-time'' and {\bf non-causal} signal processing as ``offline''.

A {\bf multivariable} system is one that has many inputs and/or outputs. Think of a showerhead: the hot and cold water valves provide two independendant inputs. Additional, a showerhead has two outputs: total flow rate and temperature. Control problems involving these systems can be much more complicated that scalar systems (single-input single-output system).

A system is linear if
\begin{align*}
y_1 = Sf_1 \\
y_2 = Sf_2
\end{align*}
implies \[ y_1 + y_2 = S(f_1 + f_2) \] Additionally, if $c \in \mathbb{C}$, \[ S(cf_1) = cy_1 \]

More formally, a system is linear if a linear combination of inputs outputs a linear combination of those inputs' responses: \[ S(c_1f_1 + c_2f_2) = c_1y_1 + c_2y_2 \] for any $c_1, c_2 \in \mathbb{C}$. We call this the principle of superposition.

Roughly speaking, a system is {\bf time invariant} if its response to a signal doesn't change with time. Mathematically, \[ f(t) \xrightarrow{S} y(t) \implies f(t-T) \rightarrow y(t-T) \]

\subsection{Impulse}
The {\bf unit impulse} or {\bf Dirac delta function} was inspired by classical mechanics. We notate it as
\begin{align*}
m\dot v &= f \\
\int_{t_1}^{t_2} m\dot v \dd t &= \int_{t_1}^{t_2} f \dd t \\
mv(t_2) - mv(t_1) &= \int_{t_1}^{t_2} f \dd t
\end{align*}
Where the impulse, or change in momentum, is denoted $\displaystyle\int_{t_1}^{t_2} f \dd t$.

Note that impulse doesn't depend on the ``profile'' of $f(t)$, just on the integral $\displaystyle\int_{t_1}^{t_2} f(t)\dd t$.

The unit impulse function represents a finite impulse delivered instantaneously over an interval of length zero. This is physically impossible, but is important as a limiting case.

Intuitively, consider a rectangular pulse of width $\Delta t$. The unit impulse $\delta(t)$ represent sthe ``limiting case'' where $\Delta t \to 0$. Defining propery: the {\bf sifting property}: \[ \int_{-\infty}^\infty f(t) \delta(t) \dd t \]

As $\Delta t$ becomes sufficiently small, we get $f(t) \approx f(0)$ over $\bigl[-\frac{\Delta t}{2}, \frac{\Delta t}{2}\bigl]$. So \[ \int_{-\infty}^\infty f(t) \delta(t) = f(0) \] or more generally \[ \int_{-\infty}^\infty f(t) \delta(t-T) \dd t = f(T) \] The sifting propery, then ``picks out'' the value of $f$ at $t=T$.

Given a linear, time-invariant system, we call the response of the system to an input $f(t) = \delta(t)$ (with zero initial conditions) the {\bf impulse response}.

Example: given an object sliding along a surface with friction (and supposing $f(t) = \delta(t)$), we have
\begin{align*}
M\ddot x &= f - b\dot x \\
M\ddot x + b\dot x &= f \\
\int_{0^-}^{0^+} (M\ddot x + b\dot x) \dd t &= \int_{0^-}^{0^+} f(t) \dd t \\
&= \int_{0^-}^{0^+} \delta(t) \dd t \\
&= \int_{-\infty}^{\infty} \delta(t) \dd t \\
&= 1 \\
\int_{0^-}^{0^+} M\ddot x \dd t + \int_{0^-}^{0^+} b\dot x\dd t &= \\
\int_{0^-}^{0^+} M\ddot x \dd t &= \\
M\dot x(0^+) - M\dot x(0^-) &= \\
\dot x(0^+) = \frac{1}{M}
\end{align*}

Since $\dot x$ remains bounded, we have $x(0^+) = x(0^-)$ and so for $t > 0$, $M\ddot x + b\dot x = 0$ with $\dot x(0^+) = \frac{1}{M}$ and $x(0^+) = 0$.

Let $v = \dot x$. Then $M\dot v + bv = 0$ and for $t > 0$, $v(t) = \frac{1}{M} e^{-\frac{b}{M} t}$.

% TODO: MISSING

The last equation expresses $f$ as a linear combination of unit impulses. Suppose it is the input to an LTI system $S$ and that the impulse response of the system is $g(t)$. The response to $\delta(t-\tau)$ is then $g(t-\tau)$.

The response of $S$ to $f(\tau)\delta(t-\tau)$ is $f(\tau)g(t-\tau)$ by linearity. Finally, the response to $\displaystyle\int_{-\infty}^\infty f(\tau)\delta(t-\tau) \dd\tau$ is $\displaystyle\int_{-\infty}^\infty f(\tau)g(t-\tau) \dd\tau$ (also by linearity).

So for any LTI system with impulse response $g(t)$, the response to an arbitrary input $f(t)$ is \[ y(t) = \int_{\infty}^\infty f(\tau)g(t-\tau) \dd\tau \] the convolution of $f$ and $g$. Note that convolution is symmetric, ie. $f * g = g * f$.

\subsection{Exponential Inputs}
Let $S$ be an LTI system and let its input be $f(t) = e^{st}$, giving us $y(t) = Se^{st}$. Given a shift in time, we have $y(t-\tau) = Se^{s(t-\tau)}$ by time-invariance.

Then
\begin{align*}
y(t-\tau) &= e^{-st}y(t) \\
y(0) &= e^{-st} y(t), \forall t = \tau \\
y(t) &= y(0)e^{st}
\end{align*}

Exponential signals, then, are ``eigenfunctions'' of LTI functions.

By the convolution integral,
\begin{align*}
y(t) &= \int_{-\infty}^\infty h(\tau) f(t-\tau) \dd\tau \\
&= e^{st} \int_{-\infty}^\infty h(\tau) e^{-st} \dd\tau \\
&= e^{st} H(s)
\end{align*}
where $h$ is the impulse reponse of $f$ and $f(t) = e^{st}$.

If we have $y(t) = G(s)e^{st}$, $G(s)$ is the Laplace transform of the impulse response; ie. the {\bf transfer function}. If $s = j\omega$, then $e^{st} = e^{j\omega t} = \cos\omega t + j\sin\omega t$, so we have an imaginary frequency response $G(j\omega)$.

So, by definition, we have \[ G(j\omega) = \bigl| G(j\omega) \bigl| e^{j\angle G(j\omega)} \]

\section{Modelling}
To model a system, we:
\begin{enumerate}
\item Apply relevant physical laws, giving us a system of differential equations
\item Linearize, giving us a system of linear differential equations
\item Take Laplace transforms with initial consitions set to zero, giving us a set of linear algebraic equations
\item Solve for output in terms of input, giving us a transfer function
\item Experiment to estimate the parameters
\end{enumerate}

\section{Responses}
We can relate responses to the positions of poles of transfer functions on the s-plane. Note that poles are the roots of the denominator of the transfer function. Given \[ \frac{Y(s)}{R(s)} = \frac{C(s)P(s)}{1 + C(s)P(s)} \] the control designer can choose $C(s)$ as a root and therefore influence the poles of $\frac{Y(s)}{R(s)}$.

A standard first order system is denoted as \[ G(s) = \frac{K}{s\tau + 1} \] because the polynomial is of degree one (ie. there is only a single pole). The impulse reponse of this function is
\begin{align*}
y(t) &= \invlaplace{G(s) \times 1} \\
&= \invlaplace{\frac{K}{s\tau + 1}} \\
&= \invlaplace{\frac{\frac{K}{\tau}}{s + \frac{1}{\tau}}} \\
&= \frac{K}{\tau} \invlaplace{\frac{1}{s+\frac{1}{\tau}}} \\
&= \frac{K}{\tau} e^{-\frac{t}{\tau}} u(t)
\end{align*}

Recall that \[ \laplace{e^{-at} u(t)} = \frac{1}{s + a} \] and \[ \laplace{e^{-at} f(t)} = F(s + a) \] Note that you can clearly derive the former from the latter.

The closer the pole $-\frac{1}{\tau}$ is to the imaginary axis, the slower the response.

Given the step response
\begin{align*}
y(t) &= \invlaplace{G(s) \times \frac{1}{s}} \\
&= \invlaplace{\frac{K}{s(s\tau + 1)}} \\
&= K(1 - e^{-\frac{t}{\tau}}) u(t)
\end{align*}

It is always the case that the step response is the integral of the impulse response (by the integration of the laplace transform).

A standard first-order system with time constant $\frac{\tau}{1 + K_c K}$ and dc gain $\frac{K}{1 + K_c K}$ has \[ \frac{Y(s)}{R(s)} = \frac{\frac{K_c K}{1 + K_c K}}{s\frac{\tau}{1 + K_c K} + 1} \]

A standard second-rder system is of the form \[ H(s) = \frac{\omega n^2}{s^2 + 2\zeta \omega_n s + \omega_n^2} \] Where $\omega_n$ is the natural frquency and $\zeta$ is the damping ratio.

We'll look at the case where $0 < \zeta < 1$ (the system is {\bf underdamped}). The impulse response is
\begin{align*}
y(t) &= \invlaplace{H(s) \times 1} \\
&= \frac{\omega_n}{\sqrt{1 - \zeta^2}} e^{-\zeta\omega_n t} \sin\omega_n\sqrt{1 - \zeta^2} t
\end{align*}
and the step response is its integral
\begin{align*}
y(t) &= \invlaplace{H(s) \times \frac{1}{s}} \\
&= 1 - \frac{1}{\sqrt{1 - \zeta^2}} e^{-\zeta\omega_n t} \sin(\omega_n\sqrt{1 - \zeta^2} t + \theta)
\end{align*}
where $\theta = \cos^{-1} \zeta$. Thus the steady-state response is 1.

The poles of this system are
\begin{align*}
s &= \frac{-2\zeta\omega_n \pm \sqrt{4\zeta^2\omega_n^2 - 4\omega_n^2}}{2} \\
&= -\zeta\omega_n \pm \omega_n\sqrt{\zeta^2- 1} \\
&= -\zeta\omega_n \pm j\omega_n\sqrt{1 - \zeta^2}
\end{align*}

The larger the $\omega_n$, the faster the responses. The larger the $\theta$, the smaller the $\zeta$ and the more oscillating the response is.

The {\bf peak time} is the time a system takes to reach the first response peak and is inversely proportional to the imaginary part of the poles

The impulse response is zero if and only if \[ \omega_n \sqrt{1 - \zeta^2} t = 0, \pi, 2\pi \] which implies \[ Tp = \frac{\pi}{\omega_n \sqrt{1 - \zeta^2}} \] where $\omega_n \sqrt{1 - \zeta^2}$ is the imaginary part of the poles.

The {\bf settling time} is the earliest time after which the step response remains within a certain percentage of $y$ (eg. $T(5\%)$).

We have \[ y(t) = \bigl[ 1 - \frac{1}{\sqrt{1 - \zeta^2}} e^{-\zeta\omega_n t} \sin(\omega_n \sqrt{1 - \zeta^2} t + \theta) \bigl] u_{-1}(t) \] To approximate $Ts$, it suffices to focus on $e^{-\zeta\omega_n t}$.

Thus we have \[ e^{-\zeta\omega_n t} \approx 0.05 \iff \zeta\omega_n t = 3 \implies Ts(5\%) \approx \frac{3}{\zeta\omega_n} \] and \[ e^{-\zeta\omega_n t} \approx 0.02 \iff \zeta\omega_n t = 4 \implies Ts(2\%) \approx \frac{4}{\zeta\omega_n} \] These are good approximations for typical values of $\zeta$.

We generally aim for $\zeta \leq 0.7$ (these settling time formulas are only reasonable when $\zeta < 1$.

If some of the poles are much closer (5 to 10 times) to the imaginary axis than the others, they are considered {\bf dominant}.

The real exponentials in the responses have always features the real parts of the poles, eg $e^{-\zeta\omega_n t}$. These are decaying exponentials if and only if the real parts of the poles are negative.

We say that a single-output system is BIBO stable ifits output is bounded whenever its input is.

Theorem: An LLTI system with transfer function $G(s)$ is BIBO stable if and only if $G(s)$ is stable and proper.

$G(s)$ is {\bf stable} if all of its poles lie in $\mathbb{Re}(s) < 0$ (all the poles lie to the left of the imaginary axis).

A rational transfer function $G(s)$ is {\bf proper} if the degree of the numerator polynomial is less than or equal to that of the denominator. $G(s)$ is {\bf strictly proper} if the degree of the numerator is strictly less than that of the denominator.

The {\bf final-value theorem}: if $E(s)$ has no poles in the closed right half-plane (i.e., in $\mathbb{Re} \geq 0$), except possible a single pole at $s = 0$, then \[ \lim_{t\to\infty} e(t) = \lim_{s\to 0} eE(s) \] where $e(t)$ is the steady-state error.

We call $K_p = \lim_{s\to 0} C(s) P(c)$ the {\bf position error constant}.

\subsection{The Fundamental Stability Theorem}
Let $C = \frac{N_C}{D_C}$ (and the same for $P$ and $F$) which are ratios of coprime polynomials. Then $D_CD_PD_F + N_CN_PN_F$ is the characteristic polynomial of the system.

Suppose C, P, and F are proper and $CPF$ is strictly proper. Then the feedback is BIBO stable if and only if the chip has no roots in $\mathbb{Re}(s) \geq 0$.

Alternatively, if C, P, and F are proper and $CPF$ is strictly proper, the feedback system if BIBO stable if and only if $1 + CPF$ has no roots in $\mathbb{Re}(s) \geq 0$ and $CPF$ has no pole-zero cancellations in $\mathbb{Re}(s) \geq 0$.

\subsection{The Ruth-Horowitz Theorem}
Given a polynomial $Q(s) = a_ns^n + a_{n-1}s^{n-1} + \dots + a_o$, we construct the table
\begin{table}[ht]
\centering
  \begin{tabular}{r|llll}
  $n$     & $a_n$     & $a_{n-2}$ & $a_{n-4}$ & \dots \\
  $n - 1$ & $a_{n-1}$ & $a_{n-3}$ & $a_{n-5}$ & \dots \\
  $n - 2$ & $b_{n-2}$ & $b_{n-4}$ & $b_{n-6}$ & \dots \\
  $n - 3$ & $c_{n-3}$ & $c_{n-5}$ & \dots     & \dots \\
  \dots   & \dots     & \dots     & \dots     & \dots \\
  \end{tabular}
\end{table}
where
\begin{align*}
b_{n-2} &= \frac{-1}{a_{n-1}} \begin{vmatrix} a_n & a_{n-2} \\ a_{n-1} & a_{n-3} \end{vmatrix} \\
b_{n-4} &= \frac{-1}{a_{n-1}} \begin{vmatrix} a_n & a_{n-4} \\ a_{n-1} & a_{n-5} \end{vmatrix} \\
c_{n-3} &= \frac{-1}{b_{n-2}} \begin{vmatrix} a_n & a_{n-3} \\ b_{n-2} & b_{n-4} \end{vmatrix} \\
c_{n-5} &= \frac{-1}{b_{n-2}} \begin{vmatrix} a_n & a_{n-5} \\ b_{n-2} & b_{n-6} \end{vmatrix} \\
\end{align*}

We also fill it in with zeroes as necessary.

The roots of $Q(s)$ all lie in the ``open left half-plane'' $\mathbb{Re}(s) < 0$ if and only if all elements of the first column of the Routh Table are non-zero and have the same sign. A necessary condition for all roots to lie in $\mathbb{Re}(s) < 0$ is that all the coefficients of the polynomial are nonzero and have the same sign.

There's generally a trade-off between steady-state performance and transient performance stability. The variation of the roots of the characteristic polynomial with $K$ is shown by the {\bf root locus}.

\subsection{The Root Locus}
Suppose that a chip has the form \[ D_CD_PD_F + KN_CN_PN_F \] where $k > 0$. If $D_CD_PD_F$ and $N_CN_PN_F$ have roots in common, these will be roots of the characteristic polynomial that wil not vary with $K$. Let's ignore these fixed roots by dividing by $D_CD_PD_F$ and cancelling common roots, leaving us with \[ 1 + KG(S) = 0 \]

The form of the root lous is determined by {\bf Evan's Rules}:
\begin{enumerate}
\item The root locus consists of $n$ branches (the characteristic polynomial is of degree $n$).
\item The root locus is symmetrical about the real axis (if $s$ is a root so is $s^*$).
\item As $k\to 0$, the $n$ branches of the root locus approach the poles of $G(s)$ (the characteristic equation can be writtten $D_CD_PD_F + KN_CK_PK_F = 0$). As $K\to\infty$, $m$ of the $n$ branches tend toward the zeroes of $G(s)$.
\item As $K\to\infty$, the remaining $n-m$ branches tend toward infinity along straight-line asymptotes with angles of \[ \theta = \frac{\pm k180^\circ}{n-m}, h=1,3,5,\dots \] and a common intersection point \[ \sigma = \frac{\displaystyle\sum_{j=1}^n P_j - \displaystyle\sum_{i=1}^m z_i}{n-m} \]
\end{enumerate}

\end{document}
