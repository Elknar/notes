\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,bookmark,parskip,custom}
\usepackage[margin=.8in]{geometry}
\allowdisplaybreaks
\hypersetup{colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\setcounter{secnumdepth}{5}

\begin{document}

\title{CS 458 --- Computer Security and Privacy}
\author{Kevin James}
\date{\vspace{-2ex}Spring 2016}
\maketitle\HRule

\tableofcontents
\newpage

\section{Program Security}
Programs have bugs. Thus, security-relevant programs have security bugs.

\subsection{Flaws, Faults, and Failures}
A {\bf flaw} is a problem with a program. A {\bf security flaw} is one which affects security (confidentiality, integrity, availability) in some way. Flaws can be considered either {\bf faults} or {\bf failures}. A fault is a mistak ebehind the scenes; a potential problem that does not necessarily cause a real issue. A failure, on the other hand, is when something actually goes wrong {\it from the user's perspective}. Generally, a fault will eventually lead to a failure.

We can find faults in several ways: tracing backwards from a failure, trying to cause failures to trace them back, static verification, etc. Once a fault has been found, small patches can be made to fix the issue. We call this ``penetrate and patch'', see Microsoft's ``Patch Tuesday''s.

Patching does not necessarily make things better -- it can actively make things worse! Patches can cause regressions, expose worse flaws, introduce new faults, etc.

Though specifications generally specify minimum required behaviour, for security- or privacy-relevant software, we consider ``and nothing else'' to be implicitly added to the spec.

\subsection{Unintentional Security Flaws}
Some flaws are intentional: {\bf malicious} flaws intended to attack systems or {\bf nonmalicious} (but intentional) flaws which are generally features that can cause a failure when attacked. Malicious flaws can be split into {\bf general} and {\bf targetted} flaws; eg. software holes vs keyloggers. Most security issues are caused by unintentional flaws.

An example of a nonmalicious flaw is the SSL Heartbleed bug. The heartbeat mechanism was missing a bounds check, so attackers could request arbitrarily large chunks of code from the SSL server's memory -- much of which may include sensitive data.

\subsubsection{Buffer Overflow Attacks}
One of the most common types of attack vectors are {\bf buffer overflow attacks}. The general idea here is a lack of bounds checking on accessing memory; if this is not properly bounds-checked, attackers can overwrite data past/on the stack to change things such as the saved return address, thus making a program jump to an address of her choice.

Sometimes, buffer overflow attacks are more restricted: off-by-one errors where only a single byte can be overwritten, overflows on the heap instead of the stack, and only being able to jump to other parts of the program (or standard libraries) instead of arbitrary shellcode.

We can defend against buffer overflows by
\begin{enumerate}
\item using a language with bounds checking
\item having a non-executable stack (writeable or executable, never both)
\item randomized stack location per-process (many OSes do this)
\item ``canary'' compiler feature to detect stack modifications
\end{enumerate}

\subsubsection{Format String Vulnerabilities}
This class of vulnerabilities was only recently (2000) discvoered: basically, any function which allows a user to specify a portion of the format string gives a small chance that the program is vulnerable. For example:

\begin{enumerate}
\item \code{printf("\%s\%s\%s\%s")} will likely crash
\item \code{printf("\%x\%x\%x\%x")} will likely dump your stack
\item \code{\%n} will {\it write} to an address on the stack
\end{enumerate}

\subsubsection{Incomplete Mediation}
{\bf Mediation} is the act of validation user-entered data to ensure it is a meaningful request. This is important when we accept input from untrusted users, eg. through webforms. A lack of mediation opens us up to problems such as incorrect data, buffer overflows, SQL injection, etc.

{\bf Client-side mediation} is a common way to validate webforms: this is a bad idea, since users can tamper with the JavaScript code used to prevent illegal inputs. Similarly, client-side state is often used to help remember users, etc. Similarly, this allows users to tamper with the state.

To defend against client-side mediation, server-side mediation is necessary. For values entered by the user, values should be carefully checked; for state stored by the client, a check against client modification is necessary. Client-side mediation, then, is not necessary and provides no security value, though it may be used to provide a friendlier user interface.

\subsubsection{TOCTTOU}
A TOCTTOU attack is a {\bf time-of-check to time-of-use} attack, which can result in a race condition where the ``check'' is no longer valid at the time of the ``use''. The problem with this is the time in between the two requests, in which an attacker can modify data in order to gain access to something which should have been restricted.

For example, consider \code{cat}. If \code{cat} simply checked the read bit and then, if it was readable, dumped the file, an attacker could run

\begin{verbatim}
$ touch /tmp/exploit
$ chmod 0777 /tmp/exploit
$ ln -s /tmp/exploit leet
$ cat leet &; ln -s /etc/shadow leet
...
root:$6$PASSWORD_HASH_HERE:16451::::::
...
\end{verbatim}

To avoid this issue, we muct make sure all information related to the access control decision is constant between the time of the check and the time of the action. Alternatively, locks can be used to ensure no change can occur mid-request.

\subsection{Intentional Security Flaws}

\subsubsection{Malware}
{\bf Malware} is software written with malicious intent. Malware does require execution to have any effect or cause harm. This often done thorugh user action, eg. a user downloading and running malicious software, inserting a USB drive with autorun enabled, etc. Additionally, it can be possible to exploit some system flaw such as buffer overflows in network daemons, email clients, web browsers, etc.

\subsubsection{Viruses}
A {\bf virus} is a particular type of malware that infects other files. Traditionally, viruses could only infect executables -- nowadays, many more formats are vulnerable. Typically, the virus activates on execution and tries to infect other files with copies of itself. In this way, a virus can spread between files and computers.

As a general rule, viruses modify other programs to copy itself to the beginning of the target program's code (rather, the virus modifies the entry to point at its code, which is at the end of the program, then jumps back to the original entrypoint; this ensures direct addressing within the original program remains possible). They are generally non-destructive to the file so as to remain undetected.

Additionally, a virus' payload may disable virus scanning software, erase your harddrive, install keyloggers, create a botnet, or start attacking some particular other service or website.

We can protect against viruses through several methods: the most common is to keep a list of known viruses along with some {\bf signature} for identifying them. This signature is generally the infectoin code or payloadd code. We can also identify patterns characteristic of a particular virus, such as where it tries to hide itself and how it propgates.

To avoid this protection, many viruses use {\bf polymorphism}, making slight modifications of itself at each duplication. This is often done through code encryption; by using a different encryption key at each copy, the resulting code has a different checksum. In this case, the decryption code is often used aas the signature for the virus.

\subsubsection{Trojan Horses}
{\bf Trojan horses} are programs which claim to do something innocuous (and usually do) but which also hide malicious behaviour. These must be programs which the user actually wants to run. The payload of a trojan can be anything; a virus, for example. That said, a trojan doesn't itself usually spread between computers; they instead rely on multiple users executing the software.

{\bf Scareware} is a program which appears to be a warning which convinces you to install some other payload such as a virus. These are often websites that resize your browser to appear as system dialogs warning of viruses.

{\bf Ransomware} generally encrypts your hard-drive until you send them a payment.

A {\bf logic bomb} is malicious code hiding in software already on your computer, waiting for some event to occur. They are often placed by insiders, ie. employees leaving the company. The payload is generally pretty dire: they might erase data, corrupt data, install ransomeware, etc. Either way, te trigger is usually something the insider can effect once he is no longer an insider: on some rare event, when a special sequence is entered on the public aspect of the company's code, or at some given time in the future (``time bombs'').

Spotting torjans and logic bombs is very tricky since they, respectively, appear to be normal programs or appear to do nothing. Since the user is intentionally running the code, it's hard to detect that these could be malicious.

\subsubsection{Worm}
A {\bf worm} is a self-contained piece of code that can replicate with little or no user involvement. They often use security flaws in widely deployed software as a path to infection. Typically, they exploit some flaw to gain access to a computer, then starts scanning for other computers to infect. There may or may not be some payload that activates at some point, but this is not necessary.

\subsubsection{Web Bugs}
A {\bf web bug} is an object (usually a 1x1 pixel transparent object) embedded in a web page which is feteched from a different server than the one that originally served the page. This can send information about you to third parties (often advertisers) whithout your consent.

Why do we consider this malicious code? Only because it affects privacy, rather than security. With the help of cookies like this, advertisers and other third parties can learn about the other sites you visit, learn about your identity, etc.

\subsubsection{Back Doors}
A {\bf back door} (also called a {\bf trap door}) is a set of instructions designed to bypass the normal authentication mechanisms of a system. These can often be included for debugging or testing (and are accidentally not removed!), for legal reasons, or as a malicious addition.

\subsubsection{Salami Attacks}
A {\bf salami attack} is one that is made up of many smaller, often considered inconsquential, attacks. The classic example: send the fractions of cents of round-off error from many accounts to a single account owned by the attacker. Similarly, there are attacks where credit card thieves make small charges to very many cards, clerks slightly overcharge for service, gas pumps misreport the amount of gas dispensed, etc.

\subsubsection{Rootkits}
A {\bf rootkit} is often used by skiddies. It often has two main parts: a method for gaining unauthorized root access (either locally from an unprivileged account or remotely) and a way to hide its own existence. These kits often exploit some known flaw and leave a backdoor for if/when that flaw is repaired.

Rootkits hide their presence by cleaning log messages involving themselves, modifying commands such as \code{ls} and \code{ps}, and modifying the kernel itself to hide the kit from userspace.

\subsubsection{Keyloggers}
A {\bf keystroke logger} might be installed to keep a record of messages sent, passwords, etc. These will often be stored to a disk and accessed locally though some send the data to a remote machine.

These can be application specific, system-wide, or even hardware-based.

\subsubsection{Interface Illusions}
These are often websites which appear to do one thing but actually do something else, eg. dragging a scrollbar which actually drags a program from a malicious website to your ``Startup'' folder.

Similar to this is the notion of {\bf clickjacking}, wherein, for example, a transparent button taking a malicous action is transparently overlayed upon a real button.

{\bf Phishing} is a type of interface illusion. A phishing attack generally involves pretending to be a service you are not, for example creating a website at \code{paypa1.com} which appears to be the paypal website. Advanced phishing can appear every bit like the real thing, even the URL bar and the SSL certificate.

\subsubsection{Man-in-the-Middle Attacks}
Keyboard logging (sort of), interface illusions, and phishing are examples of {\bf man-in-the-middle attacks}. Basically, these are defined by a service intercepting the user's communication with their intended target, then passing along the data normally to pretend nothing is wrong. This allows them to take some malicious action against the data being passed back-and-forth without wither side being the wiser.

\subsubsection{Side Channels}
Sometimes, attackers may use {\bf covert channels} to gain information. A good example of this is stenography, wherein unrelated data is included in an image. More generally, information can be encoded in RF emissions, power consumption, audio emissions, shared CPU caches, computation time, etc.

A timing side channel attack in the quare-and-multiply algorithm: to compute $x^d$ mod $n$, $d$ is a secret represented in binary and for each zero, do one multiplication, for each 1, do two multiplations. Statistical analysis on timings can reveal $d$ to attackers.

\section{Privilege Escalation}
Most systems have the concept of differing levels of privilege for different users; websites have global read-access and admin-only write-access, Unix lets users write to \code{\ttilde} but not \code{/usr/bin}, owners of mailing lists can perform extra tasks, etc.

A {\bf privilege escalation} is an attack which raises the privilege level of the attacker beyond that to which he would normally be entitled.

\subsection{Sources}
A privilege escalation flaw often occurs in a part of the system which legitimately runs with a higher privilege that can be tricked into executing commands with that privilege on the attacker's behalf. Examples include buffer overflows in setuid programs or network daemons and component substitution. Another way this can occur is by tricking the system into thinking one has higher privileges than is actually the case, eg. when there are problems with authentication systems (\code{-froot}).

\section{Operating System Security}
An operating system allows different users to access different resources in a shared way. The OS needs to control this sharing and provide an interface to allow this access. {\bf Identification} and {\bf authentication} are required for this access control.

% TODO: slides

\subsection{Access Control}
{\bf Access control} generally has three goals:
\begin{description}
\item [check every access] otherwise the OS might fail to notice access has been revoked
\item [enforce least privilege] grant program accesss to only the smallest number of objects required to perform a task
\item [verify acceptable use] limit types of activity that can be performed on an object
\end{description}

The {\bf access control matrix} can be used to describe access levels:
\begin{itemize}
\item the set of protected objects is $O$ (files, database records)
\item the set of subjects is $S$ (users, processes acting for them)
\item the set of rights is $R$ (read, write, execute)
\end{itemize}

Access control matrix consists of entries $a[s,o]$, where $s \in S$, $o \in O$ and $a[s,o] \subseteq R$. In practice, matrices are not used since they are incredibly sparse.

More frequently, we use {\bf access control lists}, where each object has a list of subjects and that subject's access rights. Subjects are only present in this list if they have some non-empty set of rights.

A {\bf capability} is an unforgeable tokan that gives its owner some access rights to an object. This is enforced by having the OS store and maintain tokens through cryptographic mechanisms. Tokens may be transferable, but this is not necessary.

In some cases, it may make sense to combine both ACLs and capabilities. In UNIX, for example, each file has an ACL which is consulted upon an \code{open()} attempt. If approved, the caller is given a capability listing type of access allowed in the ACL (ie. read/write). Upon calling \code{read()} or \code{write()}, the OS simply looks at the capability (stored in memory) to determine the access type allowed. Note that this violates the goal of ``checking every access''.

There also exists {\bf Role-Based Access Control} (RBAC). This is often used for, eg. companies, whereing a user's access often depends on the user's job function (role) within the company. RBAC also supports more complex access control scenarios: {\bf hierarchical} roles for, eg. managers, {\bf multiple roles} varying based on current tasks, ec. This enforces a {\bf separation of duty}.

\subsubsection{Authentication}
There are four classes of authentication factors:
\begin{itemize}
\item something the user {\bf knows}, such as a password, PIN, or secret question
\item something the user {\bf has}, such as an ATM card, badge, browser cookie, key, uniform, or phone
\item something the user {\bf is}, such as biometrics
\item something about the user's {\bf context}, such as location or time
\end{itemize}

Different classes of these factors can be combines for more solid authentication. Multiple factors from the same class doesn't really help. It is also important to know that something you have can become something you know, eg. by entering the numbers from your credit card online.

Passwords are the oldest form of authentication mechanism, but have several usability problems: unrecoverable forgotten passwords, inconvenient to enter them, disclosed passwords remove all protection, sharing passwords prevents easy updates. They are also vulnerable to many forms of attacks: shoulder surfing, keystroke logging, interface illusions, phishing, password re-use, password, guessing, et cetera.

Guessing attacks, especially on short passwords, are particularly effective: an eight character password can be brute-force attacked in no more than five hours (though this is exponential by length of password). If we order these attempts by password likelihood, this becomes even faster in the average case.

This doesn't necessarily mean we should give up on passwords, since there are many things that can be done to mitigate these issues. We also don't really have anything better: something like biometrics is a possibility, but it has its own set of problems.

\subsection{Security Policies}
Typically, a trusted operating system builds on four factors:
\begin{itemize}
\item a {\bf policy}, which is a set of rules defining what is secured and why,
\item a {\bf model} that implements this policy and can be used to reason about it,
\item a {\bf design}, which specifies how the OS implements the model, and
\item {\bf trust}, which assures the OS is implemented according to the design.
\end{itemize}

Trusted software is similar, it's been rigourously developed and analyzed and is thus trusted to do exactly what it promises and no more. For this to be effective, we must have
\begin{itemize}
\item functional correctness,
\item integrity enforcement (ie. wrong inputs don't break good data),
\item limited privilege, and
\item appropriate confidence levels (in the software)
\end{itemize}

To this end, many security policies have been developed. Particularly for industry applications, many of these are modelled upon government security models.

\subsubsection{Chinese Wall Policy}
\begin{quote}
Once you have been able to access information about a particular kind of company, you will no longer be able to access information about other companies of the same kind.
\end{quote}

For example, assume Alice and Bob are consultants for various companies: Wendy's and MacDonalds (restaurants) and Chapters and Amazon (bookstores). If Alice reads Wendy's information, she can never access MacDonalds information. Though this prevents contamination within classifications (eg. restaurants and bookstores). However, this still allows Alice to write to Chapters; if Bob reads from Chapters and writes to MacDonalds, this information can still contaminate MacDonalds. In this way, this policy provides indirect information flow violating its constraints.

\subsubsection{Lattices}
A {\bf lattice} is a dominance relationship similar to the military security model and defined with antisymmetry and transitivty. It defines a partial order such that, eg. neither $a \geq b$ nor $b \geq a$ might hold for two levels $a$ and $b$. For every $a$ and $b$ in this lattice, there exists some {\bf unique lowest upper bound} $u \geq a$ and $u \geq b$ as well as a {\bf unique greatest lower bound} $l$. There also exist two elements $U$ and $L$ that dominate / are dominated by all levels.

Consider the set $S = \{ x \suchthat x \leq a, x \leq b \}$. In this case, we have defined the lower bounds -- if there exists an element $l \in S$ such that $l \geq x$ for all $x \in S$, then $l$ is the greatest lower bound. Similarly, we can find the greatest upper bound from the upper bounds.

We define the following: $(x, A) \leq (y, B)$ if $x \geq y$ and $B \subseteq A$. This allows us to create a few other definitions:
\begin{align*}
GLB((x, A), (y, B)) &= (\min\{x,y\}, A \cap B) \\
LUB((x, A), (y, B)) &= (\max\{x,y\}, A \cup B)
\end{align*}

\subsubsection{Bell-La Padula (BLP)}
This policy regulates information flow in LMS (lattice-based) policies. The basic principle is the information can only flow up: subject $s$ can only read object $o$ if $C(s) \geq C(o)$ and $s$ can only have write access to $o$ if $C(o) \geq C(s)$.

\subsubsection{Biba Integrity Model}
This prevents inappropriate modification of data and is the dual of BLP: the opposite of the previous relationships hold, eg users can only write below their security level and can only read above that level. This prevents unreliable people from modifying high integrity information and prevents unreliable information from contaminating subjects.

The {\bf Low Watermark Property} states that Biba's access rules are very restrictive. Instead, we can use dynamic integrity levels: the {\bf subject low integrity property} says ``if subject $s$ reads object $o$, the $l(s) = GLB(l(s), l(o))$''. Similarly, if $s$ modifies $o$, then $l(o) = GLB(l(s), l(o))$. Rather, a subject's integrity can decrease as she reads low-integrity objects and an object's integrity can decrease as it is modified by low-integrity subjects. In practice, though, this eventually leads to a no-access-control system wherein all integrity becomes equal (at the loweest tier). To solve this, some method of increasing integrity must exist.

Thos both Biba and BLP are easy to reason about, they are a bit too simple for great practical benefit and must be modified heavily before they are used. Particularly, they don't provide declassification and need both confidentialty and integrity rather than just one of those. Finally, information leaks may still be possible through covert channels not enforced within these models.

\subsubsection{Information Flow Control}
An {\bf information flow policy} defines the authorized paths along which information can flow. In compiler-based information flow control, a compiler checks whether the information in a program could violate an information flow policy. This can be either explicitly (eg. $x = y$) or implicitly ($if (x == 1) { y = 1 } else { y = 0 }$).

Input parameters to progams have a lattice-based security classification associated with them. The compiler then updates the security classifcation of each variable depending on program usage (ie. with dynamic BLP/Biba). Ultimately, the compiler can then output the classficiations for each output variable. The user (or another program, etc) is only allowed to see outputs which correspond to its security policy.

\end{document}
